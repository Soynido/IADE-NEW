#!/usr/bin/env python3
"""
Script d'indexation TF-IDF des chunks
T√¢che [018] - Phase 2 : Indexation & Alignement

Objectif:
- Extraire les mots-cl√©s dominants de chaque chunk via TF-IDF
- Agr√©ger par module pour cr√©er keywords.json
- Sert de base pour le contr√¥le lexical de la g√©n√©ration

Usage:
    python scripts/index_chunks.py --modules src/data/modules/ \
                                   --out src/data/keywords.json
"""

import argparse
import json
from pathlib import Path
from typing import Dict, List
from collections import Counter

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import numpy as np
except ImportError:
    print("‚ùå D√©pendances manquantes. Installez: pip install scikit-learn numpy")
    exit(1)

# =============================================================================
# CONFIGURATION
# =============================================================================

# Stopwords m√©dicaux fran√ßais (√† ne pas consid√©rer comme mots-cl√©s)
MEDICAL_STOPWORDS = [
    "patient", "patients", "cas", "√™tre", "fait", "permet", "doit", "peut",
    "fois", "niveau", "pr√©sence", "absence", "ainsi", "donc", "notamment",
    "par", "pour", "avec", "dans", "sur", "lors", "selon", "via"
]

TOP_N_KEYWORDS_PER_CHUNK = 10
TOP_N_KEYWORDS_PER_MODULE = 50

# =============================================================================
# FONCTIONS
# =============================================================================

def extract_keywords_tfidf(texts: List[str], top_n: int = 10) -> List[List[str]]:
    """
    Extrait les top N mots-cl√©s de chaque texte via TF-IDF.
    
    Args:
        texts: Liste de textes (chunks)
        top_n: Nombre de mots-cl√©s √† extraire par texte
    
    Returns:
        Liste de listes de mots-cl√©s (une liste par texte)
    """
    if not texts:
        return []
    
    # Vectorisation TF-IDF
    vectorizer = TfidfVectorizer(
        max_features=500,
        ngram_range=(1, 2),  # Unigrammes et bigrammes
        stop_words=MEDICAL_STOPWORDS,
        lowercase=True,
        min_df=1,
        max_df=0.8
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        
        keywords_per_text = []
        
        for i, text in enumerate(texts):
            # R√©cup√®re les scores TF-IDF pour ce texte
            scores = tfidf_matrix[i].toarray()[0]
            
            # Trie les indices par score d√©croissant
            top_indices = np.argsort(scores)[::-1][:top_n]
            
            # R√©cup√®re les mots-cl√©s correspondants
            keywords = [feature_names[idx] for idx in top_indices if scores[idx] > 0]
            keywords_per_text.append(keywords)
        
        return keywords_per_text
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Erreur TF-IDF: {e}")
        return [[] for _ in texts]

def index_module(module_data: Dict, module_id: str) -> Dict:
    """
    Indexe un module complet : extrait mots-cl√©s par chunk et agr√®ge.
    
    Returns:
        {
            'module_id': str,
            'chunk_keywords': {chunk_id: [keywords]},
            'module_keywords': [top keywords],
            'chunks_count': int
        }
    """
    chunks = []
    chunk_ids = []
    
    # Collecte tous les chunks du module
    for section in module_data.get('sections', []):
        for chunk in section.get('chunks', []):
            chunks.append(chunk['text'])
            chunk_ids.append(chunk['chunk_id'])
    
    if not chunks:
        return {
            'module_id': module_id,
            'chunk_keywords': {},
            'module_keywords': [],
            'chunks_count': 0
        }
    
    # Extraction TF-IDF par chunk
    keywords_lists = extract_keywords_tfidf(chunks, TOP_N_KEYWORDS_PER_CHUNK)
    
    # Mapping chunk_id ‚Üí keywords
    chunk_keywords = {
        chunk_id: keywords
        for chunk_id, keywords in zip(chunk_ids, keywords_lists)
    }
    
    # Agr√©gation module: top mots-cl√©s les plus fr√©quents
    all_keywords = [kw for keywords in keywords_lists for kw in keywords]
    keyword_counts = Counter(all_keywords)
    module_keywords = [kw for kw, _ in keyword_counts.most_common(TOP_N_KEYWORDS_PER_MODULE)]
    
    return {
        'module_id': module_id,
        'chunk_keywords': chunk_keywords,
        'module_keywords': module_keywords,
        'chunks_count': len(chunks)
    }

# =============================================================================
# MAIN
# =============================================================================

def main():
    parser = argparse.ArgumentParser(description="Indexation TF-IDF des chunks")
    parser.add_argument('--modules', required=True, help='Dossier contenant les modules JSON')
    parser.add_argument('--out', required=True, help='Fichier keywords.json de sortie')
    
    args = parser.parse_args()
    
    print("="*60)
    print("INDEXATION TF-IDF DES CHUNKS")
    print("="*60)
    
    modules_dir = Path(args.modules)
    
    # Trouve tous les modules
    module_files = list(modules_dir.glob("*.json"))
    module_files = [f for f in module_files if f.stem != 'reclassification_proposals']
    
    print(f"\nüìÅ {len(module_files)} modules trouv√©s")
    
    # Indexation de chaque module
    indexed_modules = {}
    total_chunks_indexed = 0
    total_chunks_with_keywords = 0
    
    for module_file in sorted(module_files):
        module_id = module_file.stem
        
        with open(module_file, 'r', encoding='utf-8') as f:
            module_data = json.load(f)
        
        print(f"\nüìä Indexation module: {module_id}")
        indexed = index_module(module_data, module_id)
        
        if indexed['chunks_count'] == 0:
            print(f"   ‚ö†Ô∏è  Module vide, skip")
            continue
        
        indexed_modules[module_id] = indexed
        
        chunks_with_kw = sum(1 for kws in indexed['chunk_keywords'].values() if len(kws) >= 3)
        total_chunks_indexed += indexed['chunks_count']
        total_chunks_with_keywords += chunks_with_kw
        
        coverage_percent = (chunks_with_kw / indexed['chunks_count'] * 100) if indexed['chunks_count'] > 0 else 0
        
        print(f"   ‚úì {indexed['chunks_count']} chunks index√©s")
        print(f"   ‚úì {len(indexed['module_keywords'])} mots-cl√©s module")
        print(f"   ‚úì {chunks_with_kw}/{indexed['chunks_count']} chunks avec ‚â•3 mots-cl√©s ({coverage_percent:.1f}%)")
        
        # Affiche top 10 mots-cl√©s du module
        if indexed['module_keywords'][:10]:
            print(f"   Top mots-cl√©s: {', '.join(indexed['module_keywords'][:10])}")
    
    # Stats globales
    global_coverage = (total_chunks_with_keywords / total_chunks_indexed * 100) if total_chunks_indexed > 0 else 0
    
    print("\n" + "="*60)
    print("üìä STATISTIQUES GLOBALES")
    print("="*60)
    print(f"Modules index√©s : {len(indexed_modules)}")
    print(f"Chunks index√©s : {total_chunks_indexed}")
    print(f"Chunks avec ‚â•3 mots-cl√©s : {total_chunks_with_keywords} ({global_coverage:.1f}%)")
    
    # Validation du seuil
    if global_coverage >= 80:
        print(f"‚úÖ Objectif atteint (‚â•80% chunks avec ‚â•3 mots-cl√©s)")
    else:
        print(f"‚ö†Ô∏è  Objectif non atteint : {global_coverage:.1f}% < 80%")
        print(f"   Suggestion: ajuster stopwords ou ngram_range")
    
    # Sauvegarde keywords.json
    output_path = Path(args.out)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(indexed_modules, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Mots-cl√©s sauvegard√©s : {args.out}")
    
    print("\n" + "="*60)
    print("‚úÖ INDEXATION TERMIN√âE")
    print("="*60)
    
    return 0

if __name__ == "__main__":
    exit(main())

